default:
  # the random seed
  seed: 100
  # the number of parallel environments
  num_envs: 16
  # the maximum length of an episode
  max_episode_length: 1000

  # *------------------------------- training -------------------------------* #
  # learning rate for the optimizer
  learning_rate: 0.0003
  # clip epsilon for ppo loss
  clip_epsilon: 0.2
  # entropy coefficient for ppo loss
  entropy_coefficient: 0.01
  # reward scaling
  reward_scaling: 10.0
  # cost scaling
  cost_scaling: 10.0
  # gae lambda
  gae_lambda: 0.95
  # discounting factor
  discount_gamma: 0.99

  # number of epochs
  num_epochs: 800
  # number of training steps per epoch to roll out training batch
  num_training_steps_per_epoch: 20
  # rollout length
  rollout_length: 5
  # number of updates using the same batch
  num_updates_per_step: 4
  # number of minibatches to split the batch into
  num_minibatches: 32
  # size of each minibatch
  minibatch_size: 24
  # evaluation frequency
  eval_frequency: 1

  # *----------------------------- actor network ----------------------------* #
  actor_config:
    # the hidden layer sizes for the actor network
    hidden_layer_sizes: [64, 64]
    # the activation function for the actor network
    activation: relu

  # *---------------------------- critic network ----------------------------* #
  critic_config:
    # the hidden layer sizes for the critic network
    hidden_layer_sizes: [64, 64]
    # the activation function for the critic network
    activation: relu

  # *------------------------------- lagrange -------------------------------* #
  lagrange_config:
    # Tolerance of constraint violation
    threshold: 200.0
    # Initial value of lagrangian multiplier
    multiplier_init: 0.001
    # Learning rate of lagrangian multiplier
    multiplier_lr: 0.001

velocity-ball:
  # the random seed
  seed: 0
  # the number of parallel environments
  num_envs: 16
  # the maximum length of an episode
  max_episode_length: 1000

  # *------------------------------- training -------------------------------* #
  # learning rate for the optimizer
  learning_rate: 0.0003
  # clip epsilon for ppo loss
  clip_epsilon: 0.2
  # entropy coefficient for ppo loss
  entropy_coefficient: 0.01
  # reward scaling
  reward_scaling: 10.0
  # cost scaling
  cost_scaling: 10.0
  # gae lambda
  gae_lambda: 0.95
  # discounting factor
  discount_gamma: 0.99

  # number of epochs
  num_epochs: 800
  # number of training steps per epoch to roll out training batch
  num_training_steps_per_epoch: 20
  # rollout length
  rollout_length: 5
  # number of updates using the same batch
  num_updates_per_step: 4
  # number of minibatches to split the batch into
  num_minibatches: 32
  # size of each minibatch
  minibatch_size: 24
  # evaluation frequency
  eval_frequency: 1

  # *----------------------------- actor network ----------------------------* #
  actor_config:
    # the hidden layer sizes for the actor network
    hidden_layer_sizes: [64, 64]
    # the activation function for the actor network
    activation: relu

  # *---------------------------- critic network ----------------------------* #
  critic_config:
    # the hidden layer sizes for the critic network
    hidden_layer_sizes: [64, 64]
    # the activation function for the critic network
    activation: relu

  # *------------------------------- lagrange -------------------------------* #
  lagrange_config:
    # Tolerance of constraint violation
    threshold: 200.0
    # Initial value of lagrangian multiplier
    multiplier_init: 0.001
    # Learning rate of lagrangian multiplier
    multiplier_lr: 0.0035
